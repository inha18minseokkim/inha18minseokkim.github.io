---
title: "Kafka - Avro를 활용한 직렬 역직렬화(20241028)"
date: 2024-10-28
tags: [미지정]
category: 기타
---
현재는 openApi - Kafka - MSA 연동방식을 String 형식으로 사용중. 사유는 다음과 같다.
1. 현재는 api - kafka 연동 시 대용량의 데이터가 빈번하게 왔다갔다 하지않음. 그냥 데이터 수신용이니 많아봤자 평균잡아 일 1tps도 안된다.
2. openApi 담당자분께서 그냥 json으로 하자고 하심
3. Avro 구조 정하고 그럴 시간 없이 그냥 합을 맞춰야 하니


그런데 위 두 주제를 놓고 볼 때, 앞으로는 필요할 예정. 왜냐?
1. 대외계 > kafka > MSA 가 아닌 MSA > EAI > 대외계 > EAI > MSA > KAFKA 로 구조를 바꿀 것이라 내가 조금 고생하면 Avro를 활용한 Serde 최적화가 가능함
  1. [Schema evolution in Avro, Protocol Buffers and Thrift — Martin Kleppmann’s blog](https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html)
  - 위 글에서 보는것처럼 장기적으로 파이프라인 규모가 커진다면 하긴해야함. 
  - 일단 지금 당장 필요해요? 라고 물어보면 그건 아님.
2. 현재 gateway에서 계정계 컴플라이언스 요건을 만족시키기 위한 로그 테이블 적재 > 카프카로 옮길 텐데 단위업무당 하루 평균 거래건수 200만건 잡고 * n 단위업무임. 최소 하루에 2천만건 잡아도 될듯. 빈도도 많고 배치 처리가 필요 할 수도 있음. Serde 필요함
근데 일단 1번은 차치하고 2번은 필요할 확률이 상당히 높으니 먼저 가서 공부해놓고 있기로 함.
IDC 카프카는 컨플루언트이므로 나도 컨플루언트 avro, 플러그인 사용

```java
plugins {
	id 'java'
	id 'org.springframework.boot' version '3.2.4'
	id 'io.spring.dependency-management' version '1.1.4'
	id "com.github.davidmc24.gradle.plugin.avro" version "1.9.1"
}

group = 'com.example'
version = '0.0.1-SNAPSHOT'

java {
	sourceCompatibility = '17'
}

repositories {
	mavenCentral()
	gradlePluginPortal()
	maven {
		url "https://packages.confluent.io/maven"
	}
}

ext {
	set('springCloudVersion', "2023.0.0")
}

dependencies {
	implementation 'org.springframework.cloud:spring-cloud-starter-circuitbreaker-resilience4j'
	implementation 'org.springframework.boot:spring-boot-starter-data-redis'
	testImplementation 'org.springframework.boot:spring-boot-starter-test'
	implementation 'org.springframework.cloud:spring-cloud-starter-gateway'
	testImplementation 'org.springframework.cloud:spring-cloud-starter-contract-stub-runner'
	compileOnly 'org.projectlombok:lombok'
	annotationProcessor 'org.projectlombok:lombok'
	implementation 'io.netty:netty-resolver-dns-native-macos:4.1.68.Final:osx-aarch_64'
	implementation 'org.springframework.boot:spring-boot-starter-cache'
	implementation 'org.apache.avro:avro:1.11.0' // Latest version
	implementation 'io.confluent:kafka-avro-serializer:6.1.0'
	implementation 'org.springframework.kafka:spring-kafka' // Optional
}

dependencyManagement {
	imports {
		mavenBom "org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}"
	}
}

tasks.named('test') {
	useJUnitPlatform()
}
generateAvroJava {
	source("src/main/resources/avro/")
	include("**/*.avsc")
	outputDir = file("build/generated/avro-java")
}
// Option 1: configure compilation task (avro plugin will automatically match)
tasks.withType(JavaCompile).configureEach {
	options.encoding = 'UTF-8'
}
// Option 2: just configure avro plugin
avro {
	outputCharacterEncoding = "UTF-8"
	createSetters = true
}
```

avsc 파일은 MCI 기준으로 나중에 작성하고 일단 예제니깐 uuid 호출내용 호출일시 정도만 적어

```java
{
  "type": "record",
  "name": "LogTopic",
  "namespace": "com.example.apigatewaydemo.model",
  "fields": [
    {"name": "uuid", "type": "string"},
    {"name": "registerDate", "type": "string"},
    {"name": "requestBody", "type": "string"}
  ]
}
```

serde 구현

```java
public class AvroSerializer<T extends SpecificRecordBase> implements Serializer<T> {
    @Override
    public byte[] serialize(String topic, T data) {
        byte[] result = null;

        try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {
            BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(out, null);
            SpecificDatumWriter<T> writer = new SpecificDatumWriter<>(data.getSchema());
            writer.write(data, encoder);
            encoder.flush();
            result = out.toByteArray();
        } catch (IOException e) {
            e.printStackTrace();
        }
        return result;
    }
}
```


```java
public class AvroDeserializer<T extends SpecificRecordBase> implements Deserializer<T> {
    private final Class<T> targetType;

    public AvroDeserializer(Class<T> targetType) {
        this.targetType = targetType;
    }

    @Override
    public T deserialize(String topic, byte[] data) {
        T result = null;

        try {
            BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(data, null);
            SpecificDatumReader<T> reader = new SpecificDatumReader<>(targetType);
            result = reader.read(null, decoder);
        } catch (IOException e) {
            e.printStackTrace();
        }
        return result;
    }
}
```


```java
spring:
  application:
    name: demo
  data:
    redis:
      host: localhost
      port: 6379
  kafka:
    bootstrap-servers: 부트스트랩서버아이피:29092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: com.example.apigatewaydemo.serializer.AvroSerializer
    consumer:
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: com.example.apigatewaydemo.deserializer.AvroDeserializer

server:
  port: 8123
logging:
  level:
    root: INFO
    #org.springframework.web: DEBUG
    #org.springframework.cloud.gateway: DEBUG
    #org.springframework.data.redis: DEBUG

```

SERDE 구현해놓은것 등록해줌

```java
@Slf4j
@Component
@RequiredArgsConstructor
public class CustomFilter implements GatewayFilter, Ordered {
    private final LogTopicProducer logTopicProducer;
    @Override
    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
        JsonNode attribute = exchange.getAttribute(ServerWebExchangeUtils.CACHED_REQUEST_BODY_ATTR);//문자열로 받아옴
        log.info("{}",attribute);
        LogTopic logTopic = new LogTopic();
        logTopic.setUuid(UUID.randomUUID().toString());
        logTopic.setRegisterDate(LocalDateTime.now().toString());
        logTopic.setRequestBody(attribute.toString());
        log.info("{}",logTopic);
        logTopicProducer.produceLog(logTopic);
        return chain.filter(exchange);
    }

    @Override
    public int getOrder() {
        return RouteToRequestUrlFilter.ROUTE_TO_URL_FILTER_ORDER + 1;
    }
}

```

라우팅하는 중간에 캐시에서 바디 꺼내서 produce 하면됨.
가급적이면 stream 블록하지말고 바로 넣고싶은데 어차피 바디 까서 url 조립해야하므로 블록은 불가피함.

```java
@Configuration
@Slf4j
@RequiredArgsConstructor
public class RouteConfig {
    private final CacheRequestBodyFilter cacheRequestBodyFilter;
    @Autowired
    CustomFilter customFilter;
    @Bean
    public RouteLocator SimpleRouteLocator(RouteLocatorBuilder builder) {
        return builder.routes()
                .route("stock-service", p-> p.path("/api-gateway/**")
                                .filters((f->f
                                    .cacheRequestBody(JsonNode.class)//일단 샘플이니 문자열로 캐싱(이때 flux consume)
                                    .filter(customFilter)
                                )
                                )
                        .uri("http://localhost:8082")
                        )
                .build();
    }

}
```

필터 적용 ㄱㄱ, JsonNode 캐싱 시 성능 이슈 있을까? 어차피 캐싱 후 전문 조립할때 Json으로 꺼내야하긴 해서..  큰 차이는 없을듯

스키마 레지스트리를…사용할까
