---
title: "데이터 수신 pod에 KEDA 적용"
date: 2024-09-11
tags: [미지정]
category:
  - 기타
---

[KEDA | ScaledObject specification](https://keda.sh/docs/2.14/reference/scaledobject-spec/#horizontalpodautoscalerconfig)
SRE 선생님께서 Helm chart로 KEDA 사용할 수 있도록 해주심

```java
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: {scaled-object-name}
  annotations:
    scaledobject.keda.sh/transfer-hpa-ownership: "true"     # Optional. Use to transfer an existing HPA ownership to this ScaledObject
    validations.keda.sh/hpa-ownership: "true"               # Optional. Use to disable HPA ownership validation on this ScaledObject
    autoscaling.keda.sh/paused: "true"                      # Optional. Use to pause autoscaling of objects explicitly
spec:
  scaleTargetRef:
    apiVersion:    apps/v1
    kind:          Deployment
    name:         대충이름
    envSourceContainerName: {container-name}                # Optional. Default: .spec.template.spec.containers[0]
  pollingInterval:  30                                      # Optional. Default: 30 seconds
  cooldownPeriod:   300                                     # Optional. Default: 300 seconds
  idleReplicaCount: 0                                       # Optional. Default: ignored, must be less than minReplicaCount
  minReplicaCount:  1                                       # Optional. Default: 0
  maxReplicaCount:  100                                     # Optional. Default: 100
  fallback:                                                 # Optional. Section to specify fallback options
    failureThreshold: 3                                     # Mandatory if fallback section is included
    replicas: 6                                             # Mandatory if fallback section is included
  advanced:                                                 # Optional. Section to specify advanced options
    restoreToOriginalReplicaCount: true/false               # Optional. Default: false
    horizontalPodAutoscalerConfig:                          # Optional. Section to specify HPA related options
      name: {name-of-hpa-resource}                          # Optional. Default: keda-hpa-{scaled-object-name}
      behavior:                                             # Optional. Use to modify HPA's scaling behavior
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
          - type: Percent
            value: 100
            periodSeconds: 15
  triggers:
  # {list of triggers to activate scaling of the target resource}
```

샘플은 사이트에 있는것과 비슷한데, 메타영역이나 Api 버전은 argo cd에서 공통으로 처리할 듯 하고
필요한 부분 대충 보면

```java
  pollingInterval:  30
  cooldownPeriod:   300
  idleReplicaCount: 0
  minReplicaCount:  1
  maxReplicaCount:  100            
```


```java
scaleDown:
          stabilizationWindowSeconds: 300
          policies:
          - type: Percent
            value: 100
            periodSeconds: 15
```


```java
triggers:
- type: kafka
  metadata:
    bootstrapServers: kafka.svc:9092
    consumerGroup: my-group
    topic: test-topic
    lagThreshold: '5'
    activationLagThreshold: '3'
    offsetResetPolicy: latest
    allowIdleConsumers: false
    scaleToZeroOnInvalidOffset: false
    excludePersistentLag: false
    limitToPartitionsWithLag: false
    version: 1.0.0
    partitionLimitation: '1,2,10-20,31'
    sasl: plaintext
    tls: enable
    unsafeSsl: 'false'
```

[https://keda.sh/docs/2.14/scalers/apache-kafka/](https://keda.sh/docs/2.14/scalers/apache-kafka/)

여기 정도인것같은데

1. consumer group 기준으로 offset 맨 끝에있는것 관리함
2. 토픽에 레코드 새로 들어오면 Trigger On
3. 파드가 최대 3개 hpa scale up(파티션이 세 개 이므로 컨슈머 인스턴스도 3개로 잡아)
4. 일정 시간동안 consume → db 적재 수행
  1. 국내주식 데이터의 경우 3tps로 잡으면 호출 종목 수가 4천 몇백개이므로 대략 1300초, 2X분
  2. 해외주식 데이터의 경우 3tps로 잡으면 호출 종목 수가 11000개이므로 대략 3300초, 55분
5. 적재 후 hpa scale down

이런식으로 작동하면 된다.


# 문제점 및 아쉬운점 & 향후 개선방향

처음에 토픽에 레코드가 들어오면 N초 동안 파드가 기동되어서 consume 하다가 마지막 레코드에 일정 시간이 지나면 꺼지는 식으로 작동할 줄 알았는데,
LAG가 사라지는 시점부터 HPA Scale down 되는식으로 동작하고, 파드가 꺼지니깐 LAG가 쌓이고 LAG가 쌓이니깐 다시 켜져서 LAG를 없애는 식으로 동작을 한다.
컨슈머 그룹에서 마지막 오프셋 관점으로 봤을 때 보면 이게 맞긴한데, 내가 만든 컨슈머 프로그램은 떠있는 데몬형식으로 만든 프로그램임. 약간 결이 다른것 같다.
KEDA를 사용해 파드를 기동해서 데이터 적재를 하는 프로그램을 만드는 경우 데몬 형식의 리스너를 만드는 것이 아니라, 특정 오프셋 기준으로 consume 하고 꺼지는 배치형식의 잡을 만드는게 더 적합해 보인다.
  - 왜냐하면 스프링 @TopicListener 는 리스너 + 처리를 담당하도록 되어있다.
  - KEDA를 사용함으로써 리스너를 KEDA가 일정부분 담당하게 되는데(토픽이 들어오면 리스너 파드를 깨우므로)
  - KEDA로 파드를 깨웠을 때 그 파드는 컨슈머 그룹 기준으로 마지막으로 컨슘된 오프셋부터 최신 오프셋까지 consume 하고 종료되는식으로 개발되는게 맞지않았나. 싶음.
다만 데이터 수신을 Kafka pub sub 모델로 구현한 것 자체가 이미 말이 안되는 상황이므로 현재 이 상태에서의 개선보다는 후에 public 망에서 개발할 수 있는 환경이 나온다면 그쪽에서 다시 개발하는 편이 나아보임.

>>>> 위 이슈는 KEDA ScaledJob으로 해결완료
